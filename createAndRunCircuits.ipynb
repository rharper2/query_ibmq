{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook to show how to run RB circuits on IBM Quantum Experience\n",
    "\n",
    "This is geared towards running 1 and 2 qubit circuits on all the qubits simultaneously as per [Efficient Learning of Quantum Noise](https://arxiv.org/abs/1907.13022).\n",
    "\n",
    "We use a random X-gate pattern at the beginning to eliminate systematic SPAM bias.\n",
    "\n",
    "Copyright Robin Harper 2020. See GitHub repo (https://github.com/rharper2/Juqst.jl) for licence.\n",
    "\n",
    "Note this may well get out of date with qiskit. Unlike the Juqst.jl repo, I am unlikely to keep this up-to date. Qiskit has (historically) changed a lot - but the basic methodology should be instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import general libraries (needed for functions)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "#Import the RB Functions\n",
    "import qiskit.ignis.verification.randomized_benchmarking as rb\n",
    "\n",
    "#Import Qiskit classes \n",
    "import qiskit\n",
    "import time\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import execute\n",
    "from qiskit import __version__\n",
    "\n",
    "# random is needed for the initial X gates. In order to minimise SPAM (i.e. B=0.5/0.25 in old parlance)\n",
    "# a random mix of which state we return it to is best. Could seed this if you want.\n",
    "\n",
    "import numpy.random as rand\n",
    "\n",
    "# The rest are for saving and logging and list flattening.\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the data directory if it doesn't exist \n",
    "try: \n",
    "    os.makedirs(\"data\")\n",
    "except:\n",
    "    print()\n",
    "# Make the temp save directory if it doesn't exist    \n",
    "try: \n",
    "    os.makedirs(\"temp\")\n",
    "except:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AccountProvider for IBMQ(hub='ibm-q', group='open', project='main')>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us access to our IBMQ account.\n",
    "# I am assuming you have installed qiskit and have saved your credentials.\n",
    "# There are plenty of qiskit tutorials out there\n",
    "\n",
    "from qiskit import IBMQ\n",
    "IBMQ.load_account() # Load account from disk\n",
    "IBMQ.providers()    # List all available providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = IBMQ.get_provider(hub='ibm-q')\n",
    "backends=[i for i in provider.backends()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the various functions we will need to do the runs and save them\n",
    "\n",
    "We could move this into a seperate file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With each sequence run we are going to add some random bitflip gates to get rid of a SPAM parameter\n",
    "# We need to save this 'mask' with each pickle we get back.\n",
    "\n",
    "# This just makes it easier to pickle the bit pattern used to randomise the 'expected' measurements\n",
    "class savedPair:\n",
    "    def __init__(self, bits,result):\n",
    "        self.bits = bits\n",
    "        self.result = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.providers import JobStatus\n",
    "\n",
    "\n",
    "def savePairs(bs,js,start,savePrefix,forced = False):\n",
    "    \"\"\"\n",
    "    savePairs(bs,js,start,savePrefix,forced = False)\n",
    "    \n",
    "    Designed to save jobs as and when the results become available.\n",
    "    We don't want to block, however, rather we will iterate through a list\n",
    "    and try to deal gracefully with jobs that are not ready or that\n",
    "    give us an error when we try and retrieve them.\n",
    "    If the file is there, the job will be skipped UNLESS forced is true.\n",
    "    As long as we have the job_id (which should have been saved seperately)\n",
    "    we should be able to retrieve them later anyway.\n",
    "    bs: the bitstrings associated with the jobs\n",
    "    js: a list of the jobs we have\n",
    "    start: the number the jobs start at\n",
    "    savedPrefix: What do we want the saved pickles to start with.\n",
    "    forced: Set it to true if you want to save the job, even if we think it has been saved before.\n",
    "    \"\"\"\n",
    "    for (idx,j) in enumerate(js):\n",
    "        saved_this_one = False\n",
    "        bits = bs[idx]\n",
    "        if forced or not os.path.isfile(savePrefix+str(start+idx)+'.pickle'):\n",
    "           try:\n",
    "              if j.status() == JobStatus.DONE:\n",
    "                 res = j.result()\n",
    "                 with open(savePrefix+str(start+idx)+'.pickle','wb') as f:\n",
    "                        pickle.dump(savedPair(bits,res),f)\n",
    "                 saved_this_one = True\n",
    "           except ApiError as ex:\n",
    "              print(\"There was a api exception error\")\n",
    "              template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "              message = template.format(type(ex).__name__,ex.args)\n",
    "              print(message)\n",
    "              print(\"Ignoring this for now, but NOT saved.\")\n",
    "           except Exception as ex2:\n",
    "              print(\"There was a non-api exception error\")\n",
    "              template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "              message = template.format(type(ex2).__name__,ex2.args)\n",
    "              print(message)\n",
    "              print(\"Ignoring this for now, but NOT saved.\")\n",
    "           if saved_this_one == True:\n",
    "              now = datetime.datetime.now()\n",
    "              print(\"Saved: \"+str(idx+start)+\": \" + now.strftime(\"%d %B: %r \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringifyBits(bits):\n",
    "    \"\"\" Takes an array of bits (i.e. 1 or 0) and turns them into a string \"\"\"\n",
    "    return [''.join([str(x) for x in i[::-1]]) for i in bits]\n",
    "\n",
    "def mashable(s1,s2):\n",
    "    \"\"\" XOR on strings \"\"\"\n",
    "    s3 = ''\n",
    "    for i in range(len(s1)):\n",
    "        if s1[i] == s2[i]:\n",
    "            s3 = s3 + '0'\n",
    "        else:\n",
    "            s3 = s3 + '1'\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't specify an initial layout MAKE SURE the qiskit transpiler doesn't mix and match the qubit location.\n",
    "def generateInitialLayout(q):\n",
    "  \"\"\" Pass in the qregister and generate a one to one initial layout \"\"\"\n",
    "  initial_layout = {}\n",
    "  for (idx,q) in enumerate(q):\n",
    "    initial_layout[q] = idx\n",
    "  return initial_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createARun(qubits,lengths,batching,sets): \n",
    "    \"\"\"\n",
    "      createARun(qubits,lengths,batching,sets)\n",
    "      \n",
    "      Generates RB runs\n",
    "      qubits: number of qubits\n",
    "      lengths: list of lengths for the runs\n",
    "      batching: How many complete seqeunces of runs (total submissions will = len(lengths)*batching)\n",
    "      sets: how you want the qubits done e.g. [[0,1],[2],[3]] will be a 4 qubit, 2 qubit RB on 0,1 and single qubit on 2 and 3\n",
    "      \n",
    "      Returns: a tuple(bits,circuits,layout)\n",
    "               bits = patterns of x gates used. Note this is in [Q0,Q1,Q2...] order\n",
    "               circuits = the circuits to run - as a single array not nested\n",
    "               initial_layout - the layout to be passed to the execute functions.\n",
    "      \"\"\"\n",
    "    # Generate RB circuits \n",
    "    # number of qubits\n",
    "    nQ=qubits\n",
    "    rb_opts = {}\n",
    "    # Number of Cliffords in the sequence\n",
    "    rb_opts['length_vector'] = lengths\n",
    "    # Number of random sequences to batch up\n",
    "    rb_opts['nseeds'] = batching\n",
    "    #Default pattern\n",
    "    rb_opts['rb_pattern'] = sets\n",
    "\n",
    "    rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)    \n",
    "\n",
    "    initial_layout=generateInitialLayout(rb_circs[0][0].qregs[0])\n",
    "    \n",
    "    # Generate random bit pattern to apply X - gates.\n",
    "    # We need one for each seqeunce. and each batch\n",
    "    \n",
    "    bits= [[rand.randint(0,2,nQ) for _ in range(len(lengths))] for _ in range(batching)]\n",
    "    \n",
    "    \n",
    "    # Use the bit pattern in bits to apply X gates before the circuits generated.\n",
    "    for batch in range(batching):\n",
    "        for (ib,b) in enumerate(bits[batch]):\n",
    "            qregs = rb_circs[batch][ib].qregs\n",
    "            cregs = rb_circs[batch][ib].cregs\n",
    "            # Create a temporary circuit sharing the qubits and creg with our RB circuit\n",
    "            p=qiskit.QuantumCircuit(*qregs,*cregs)\n",
    "            # Step through the random array and if we have a 1 put an X circuit on the qubit of that circuit.\n",
    "            for (q,doX) in enumerate(b):\n",
    "                if doX ==1:\n",
    "                    p.x(q)\n",
    "                    # print(ib,b,q,\"setting to x\")\n",
    "            # Go throught the gates on the temporary circuit and add them to the beginning of our RB circuit.\n",
    "            for t in p.data:\n",
    "                rb_circs[batch][ib].data.insert(0,t)\n",
    "    flatCircuits = list(chain(*rb_circs))\n",
    "    return (bits,flatCircuits,initial_layout)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send(circuits,initial_layout,backend,shots=1024):\n",
    "    \"\"\"\n",
    "      backend: The backend you want\n",
    "      shots: the number of shots to take.\n",
    "      \n",
    "      returns: job created and sent.\n",
    "    \"\"\"\n",
    "    # This will not be idiomatic Python - should find out `better` way to write this\n",
    "    # Basically if we don't get an error free response from a backend query, sleep and try again.\n",
    "    max_credits=5\n",
    "    errorFree = False\n",
    "    coupling_map = backendToUse.configuration().coupling_map\n",
    "    # If its down wait for it to come back up. \n",
    "    # This attempts to stop everything going horribly wrong cause the machine is down for e.g. maintenance.\n",
    "    while not errorFree:\n",
    "        try:\n",
    "            while not backend.status().operational:\n",
    "                now = datetime.datetime.now()\n",
    "                print(\"Device not operational at\", now.strftime(\"%d %B: %r\"), \", sleeping for a bit\")\n",
    "                time.sleep(60*10) # 10 minutes.\n",
    "            errorFree = True\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keboard interrupt whilst waiting for the backend to come back up\")\n",
    "            print(\"Going to return with what has been done so far\")\n",
    "            return (jobsList,bitFlips)\n",
    "        except Exception as ex:\n",
    "            print(\"Got an exception whilst checking the backend operational status. It is almost certainly just a maintenance issue.\")\n",
    "            template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__,ex.args)\n",
    "            print(message)\n",
    "            print(\"Ill sleep for 30 minutes and then recheck.\")\n",
    "            time.sleep(60*30)\n",
    "    # if we run with simulator take out the coupling.\n",
    "    errorFree = False\n",
    "    while not errorFree:\n",
    "        try:\n",
    "            job_exp = execute(circuits, backend=backend, shots=shots, max_credits=max_credits,initial_layout=initial_layout,coupling_map=coupling_map)\n",
    "            errorFree=True\n",
    "        except Exception as ex:\n",
    "            print(\"Got an exception whilst trying to submit the job. Probably a too many concurrent etc.\")\n",
    "            template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__,ex.args)\n",
    "            print(message)\n",
    "            print(\"Ill sleep for 3 minutes and then recheck.\")\n",
    "            time.sleep(60*3)\n",
    "    return job_exp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAndSaveJobID(bits,job_exp,index,id_save_name=\"temp/JobIDandBit_temp_\"):\n",
    "    \"\"\"\n",
    "        Given a list of bits (x gates applied) and a list of jobs (returned from execute)\n",
    "        We retrieve the job_id and then save it with the corresponding bitpattern of x gates.\n",
    "        in a pickle starting at index and using a prefix\n",
    "        of id_save_name (default JobIDandBit_temp_)\n",
    "        \n",
    "        The purpose of this is incase something goes wrong before the\n",
    "        job itself can be saved - if we have the bits and the id we can\n",
    "        always get the job results later.\n",
    "    \"\"\"\n",
    "    # So now we want the jobid so we can pickle it. Can't pickle the job_exp as its threadlocked.\n",
    "    # We can get errors trying to get the jobid from the submitted job\n",
    "    # Try to deal with them gracefully.\n",
    "    try:\n",
    "        jobId = job_exp.job_id()\n",
    "        with open(id_save_name+str(index)+\".pickle\",'wb') as f:\n",
    "            pickle.dump(savedPair(bits,jobId),f)\n",
    "        now = datetime.datetime.now()\n",
    "        print(\"Saved bits and jobid: \",index,\":\", now.strftime(\"%d %B: %r\"))\n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__,ex.args)\n",
    "        print(message)\n",
    "        print(\"We got an error, probably timing, things might be busy job ids not available. etc\")\n",
    "        time.sleep(60*2)\n",
    "        try:\n",
    "            jobId = job_exp.job_id()\n",
    "            with open(id_save_name+str(index)+\".pickle\",'wb') as f:\n",
    "                pickle.dump(savedPair(bits,jobId),f)\n",
    "            now = datetime.datetime.now()\n",
    "            print(\"Saved bits and jobid: \",index,\": \", now.strftime(\"%d %B: %r\"))\n",
    "        except Exception as ex2:\n",
    "            print(\"That really didn't work, so just NOT SAVING, well only the bits. You will have the job in the returned lists, lose that an you have to re-run this.\")\n",
    "            template = \"An exception of type {0}. Args \\n{1!r}\"\n",
    "            message = template.format(type(ex2).__name__,ex2.args)\n",
    "            print(message)\n",
    "            # Its a bit pointless just saving the bits, but I suppose we can query the backend so here we go.\n",
    "            with open(id_save_name+str(index)+\".pickle\",'wb') as f:\n",
    "                pickle.dump(savedPair(bits,[]),f)\n",
    "    return jobId\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def countQueuedJobs(jobs):\n",
    "    \"\"\"\n",
    "    Checks how many jobs are queued, running or about to be queued\n",
    "    The 'fair use' policy appears to only allow you to stack the queue to a certain amount\n",
    "    I think different 'rights' allow different numbers to be queued..\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for j in jobs:\n",
    "        job_status = j.status()\n",
    "        if job_status == JobStatus.QUEUED or \\\n",
    "           job_status == JobStatus.INITIALIZING or \\\n",
    "           job_status == JobStatus.VALIDATING or \\\n",
    "           job_status == JobStatus.RUNNING:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendAJobWhenOk(nQ,lengths,batchSize,sets,backendToUse,start,end,maxQueue = 3,shots=1024,saveFile=\"date/Melbourne15_\"):\n",
    "    \"\"\"\n",
    "    sendAJobWhenOk(nQ,lengths,batchSize,sets,backendToUse,start,end,maxQueue = 3,saveFile=\"Melbourne15\")\n",
    "    Sends off a number of jobs (from start to end)\n",
    "    Only allows queue size to be 3 (you may want to alter if you can have more in queue)\n",
    "    Saves numbered with saveFile as a prefix.\n",
    "    You also need to supply:\n",
    "      nQ, the number of qubits\n",
    "      lengths, the sequence lenths as a list\n",
    "      batchSize, the number of experiments in a single batch - qasm limits will limit this depending on lengths\n",
    "      backendToUse, err the backend you want to use\n",
    "      start, start numbering here\n",
    "      end, end numbering here (well one before here I suppose).\n",
    "      shots: (default = 1024) number of shots per circuit\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    all_bits = []\n",
    "    all_job_ids = []\n",
    "    for toDo in range(start,end):\n",
    "        (bits,circuit,layout) = createARun(nQ,lengths,batchSize,sets) \n",
    "        good_to_go = False\n",
    "        while not good_to_go:\n",
    "            counted = countQueuedJobs(all_jobs)\n",
    "            if counted >= maxQueue:\n",
    "                print(\"We have {} jobs in queue, sleeping: {}\".format(counted,datetime.datetime.now()))\n",
    "                time.sleep(60*2)\n",
    "                savePairs(all_bits,all_jobs,1,saveFile,forced = False)\n",
    "            else:\n",
    "                good_to_go = True\n",
    "        job_exp = send(circuit,layout,backendToUse,shots=1024)\n",
    "        job_id = getAndSaveJobID(bits,job_exp,toDo)\n",
    "        all_jobs.append(job_exp)\n",
    "        all_job_ids.append(job_id)\n",
    "        all_bits.append(bits)\n",
    "    counted = countQueuedJobs(all_jobs)\n",
    "    while counted > 0:\n",
    "        print(\"Still have {} jobs running: {}. Waiting for 120 seconds.\".format(counted,datetime.datetime.now()))\n",
    "        time.sleep(2*60)\n",
    "        counted = countQueuedJobs(all_jobs)\n",
    "    savePairs(all_bits,all_jobs,1,saveFile,forced = False)\n",
    "    return (all_bits,all_jobs,all_job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial runs - here we test everything with the online simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Note:</h2>\n",
    "\n",
    "Choosing the lengths sensibly is very important for reliable results. With normal RB you have a reasonable idea of the fidelity and you can choose the lengths so the first is near the beginning (but with a high probability of success) and the second is close to the 'end' of the decay curve but **not** after we are at the maximally mixed state. ([Statistical Analysis of Randomized Benchmarking](https://arxiv.org/abs/1901.00535)) gives some guidance as to how to choose this.\n",
    "\n",
    "Here we need statistics that will allow us to fit lots of curves, with varying fidelity decays, so we need more data points, and then we can use some sensible fitting rules to work out which tail ones to through away. This is all detailed, but essentially we discount any length where the value is less than $17/64^{\\text{ths}}$ of the initial value (assuming a decay to 0, if you have a constant factor then that will need to be taken into account). Some of the eigenvalues in these near term devices will be lowish so we need a good concentration of shortish lengths. Some will be very high (the single qubit fidelities) and so we need a reasonable tail.\n",
    "\n",
    "If we use the two qubit protocol things are quite different. The good fidelities are poor and the bad fidelities are pluto. In our analysis I ended up using a single qubit/idle initial gate for the first sequence (even though this has some technical SPAM related problems) in order to get the really bad fits anchored in the right place - and we need a large number of small sequences because of the extreme decays. This is, again, detailed in the relevant workbooks in the GitHub repo (https://github.com/rharper2/Juqst.jl) for the Julia analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the lengths we want:\n",
    "lengths = [1,5,10,15,20,30,45,60,75,90,110]\n",
    "# Choose the layout we want (here 15 qubits, all single qubit)\n",
    "nQ = 15\n",
    "sets = [[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14]]\n",
    "\n",
    "\n",
    "# Choose how many to batch - this will depend on maximum qasm size, which will in turn depend\n",
    "# on the number of qubits, the lengths chosen and whether they are 1 or two qubits.\n",
    "# To high and you will get an error when you try to submit\n",
    "# Higher is more sequences per call.\n",
    "batchSize = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ibmq_qasm_simulator'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backendToUse = backends[0]\n",
    "backendToUse.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bits and jobid:  1 : 17 June: 09:49:18 AM\n",
      "Saved bits and jobid:  2 : 17 June: 09:49:48 AM\n",
      "Saved bits and jobid:  3 : 17 June: 09:50:20 AM\n",
      "Saved bits and jobid:  4 : 17 June: 09:51:03 AM\n",
      "Saved bits and jobid:  5 : 17 June: 09:51:35 AM\n",
      "Still have 1 jobs running: 2020-06-17 09:51:37.022011. Waiting for 120 seconds.\n"
     ]
    }
   ],
   "source": [
    "(all_b,all_j,all_j_id) = sendAJobWhenOk(15,lengths,3,sets,backendToUse,1,6,saveFile=\"data/Simulator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check we got the results we expected:\n",
    "To check its right, we should check that when we XOR with the X-gate pattern we get all the shots. \n",
    "\n",
    "Because the simulator has no noise, we expect the pattern 000...000 to be 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten out the Bits, so we can iterate through a list\n",
    "flatBits = list(chain(*all_b))\n",
    "flatBits = list(chain(*flatBits))\n",
    "fbs = stringifyBits(flatBits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = [j.result() for j in all_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [r.get_counts() for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatCounts = list(chain(*counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n",
      "000000000000000: 1024\n"
     ]
    }
   ],
   "source": [
    "for (ix,c) in enumerate(flatCounts):\n",
    "    for k in c.keys():\n",
    "        print(\"{}: {}\".format(mashable(fbs[ix],k),c[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on an actual Device\n",
    "\n",
    "This is always a bit more fraught, because the queues are larger, the device goes offline and sometimes (certainly in the past) jobs go missing etc - but fingers crossed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<IBMQSimulator('ibmq_qasm_simulator') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmqx2') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_16_melbourne') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_vigo') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_ourense') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_london') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_burlington') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_essex') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_armonk') from IBMQ(hub='ibm-q', group='open', project='main')>,\n",
       " <IBMQBackend('ibmq_rome') from IBMQ(hub='ibm-q', group='open', project='main')>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "backendToUse = backends[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bits and jobid:  0 : 17 June: 08:41:27 PM\n",
      "Saved bits and jobid:  1 : 17 June: 08:42:02 PM\n",
      "Saved bits and jobid:  2 : 17 June: 08:42:38 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:43:43.826976\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:45:48.050305\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:47:52.701443\n",
      "Saved: 1: 17 June: 08:50:03 PM \n",
      "Saved: 2: 17 June: 08:50:07 PM \n",
      "Saved bits and jobid:  3 : 17 June: 08:50:36 PM\n",
      "Saved bits and jobid:  4 : 17 June: 08:52:08 PM\n",
      "Saved bits and jobid:  5 : 17 June: 08:52:52 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:53:13.150042\n",
      "Saved: 3: 17 June: 08:55:19 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:55:22.776772\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 20:57:26.790984\n",
      "Saved: 4: 17 June: 08:59:33 PM \n",
      "Saved bits and jobid:  6 : 17 June: 09:05:34 PM\n",
      "Saved bits and jobid:  7 : 17 June: 09:06:22 PM\n",
      "Saved bits and jobid:  8 : 17 June: 09:08:18 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:08:44.669956\n",
      "Saved: 5: 17 June: 09:10:48 PM \n",
      "Saved: 6: 17 June: 09:10:50 PM \n",
      "Saved: 7: 17 June: 09:10:54 PM \n",
      "Saved bits and jobid:  9 : 17 June: 09:14:14 PM\n",
      "Saved bits and jobid:  10 : 17 June: 09:15:08 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:15:37.008717\n",
      "Saved: 8: 17 June: 09:17:39 PM \n",
      "Saved: 9: 17 June: 09:17:43 PM \n",
      "Saved bits and jobid:  11 : 17 June: 09:19:10 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:19:38.552177\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:21:43.178581\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:23:47.223957\n",
      "Saved: 10: 17 June: 09:25:51 PM \n",
      "Saved: 11: 17 June: 09:25:55 PM \n",
      "Saved bits and jobid:  12 : 17 June: 09:27:33 PM\n",
      "Saved bits and jobid:  13 : 17 June: 09:28:32 PM\n",
      "Saved bits and jobid:  14 : 17 June: 09:29:27 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:29:52.928233\n",
      "Saved: 12: 17 June: 09:31:55 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:31:59.915106\n",
      "Saved: 13: 17 June: 09:34:04 PM \n",
      "Saved bits and jobid:  15 : 17 June: 09:34:35 PM\n",
      "Saved bits and jobid:  16 : 17 June: 09:39:15 PM\n",
      "Saved bits and jobid:  17 : 17 June: 09:39:46 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:39:59.383277\n",
      "Saved: 14: 17 June: 09:42:01 PM \n",
      "Saved: 15: 17 June: 09:42:03 PM \n",
      "Saved: 16: 17 June: 09:42:07 PM \n",
      "Saved bits and jobid:  18 : 17 June: 09:43:27 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:43:39.981444\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:45:43.981010\n",
      "Saved: 17: 17 June: 09:47:47 PM \n",
      "Saved: 18: 17 June: 09:47:50 PM \n",
      "Saved bits and jobid:  19 : 17 June: 09:49:10 PM\n",
      "Saved bits and jobid:  20 : 17 June: 09:49:42 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:49:53.803355\n",
      "Saved: 19: 17 June: 09:51:57 PM \n",
      "Saved bits and jobid:  21 : 17 June: 09:52:18 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:52:29.739192\n",
      "Saved: 20: 17 June: 09:54:33 PM \n",
      "Saved bits and jobid:  22 : 17 June: 09:54:53 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 21:55:04.113911\n",
      "Saved: 21: 17 June: 09:57:08 PM \n",
      "Saved bits and jobid:  23 : 17 June: 10:01:37 PM\n",
      "Saved bits and jobid:  24 : 17 June: 10:02:04 PM\n",
      "Saved bits and jobid:  25 : 17 June: 10:02:29 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:02:41.024952\n",
      "Saved: 22: 17 June: 10:04:43 PM \n",
      "Saved: 23: 17 June: 10:04:45 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:04:49.072196\n",
      "Saved: 24: 17 June: 10:06:53 PM \n",
      "Saved: 25: 17 June: 10:06:56 PM \n",
      "Saved bits and jobid:  26 : 17 June: 10:07:16 PM\n",
      "Saved bits and jobid:  27 : 17 June: 10:08:13 PM\n",
      "Saved bits and jobid:  28 : 17 June: 10:08:51 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:09:02.602324\n",
      "Saved: 26: 17 June: 10:11:05 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:11:09.661808\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:13:13.533644\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:15:17.596232\n",
      "Saved: 27: 17 June: 10:17:21 PM \n",
      "Saved bits and jobid:  29 : 17 June: 10:17:50 PM\n",
      "Saved bits and jobid:  30 : 17 June: 10:19:30 PM\n",
      "Saved bits and jobid:  31 : 17 June: 10:20:03 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:20:13.938254\n",
      "Saved: 28: 17 June: 10:22:16 PM \n",
      "Saved: 29: 17 June: 10:22:18 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:22:22.296190\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:24:26.589791\n",
      "Saved: 30: 17 June: 10:26:31 PM \n",
      "Saved: 31: 17 June: 10:26:34 PM \n",
      "Saved bits and jobid:  32 : 17 June: 10:26:51 PM\n",
      "Saved bits and jobid:  33 : 17 June: 10:27:14 PM\n",
      "Saved bits and jobid:  34 : 17 June: 10:31:38 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:31:49.335916\n",
      "Saved: 32: 17 June: 10:33:51 PM \n",
      "Saved: 33: 17 June: 10:33:55 PM \n",
      "Saved bits and jobid:  35 : 17 June: 10:34:11 PM\n",
      "Saved bits and jobid:  36 : 17 June: 10:34:36 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:34:46.923484\n",
      "Saved: 34: 17 June: 10:36:49 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:36:52.584114\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:38:56.937315\n",
      "Saved: 35: 17 June: 10:41:00 PM \n",
      "Saved: 36: 17 June: 10:41:03 PM \n",
      "Saved bits and jobid:  37 : 17 June: 10:42:10 PM\n",
      "Saved bits and jobid:  38 : 17 June: 10:42:35 PM\n",
      "Saved bits and jobid:  39 : 17 June: 10:44:02 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:44:13.041212\n",
      "Saved: 37: 17 June: 10:46:15 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:46:19.073451\n",
      "Saved: 38: 17 June: 10:48:22 PM \n",
      "Saved bits and jobid:  40 : 17 June: 10:48:40 PM\n",
      "Saved bits and jobid:  41 : 17 June: 10:51:32 PM\n",
      "Saved bits and jobid:  42 : 17 June: 10:51:59 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:52:09.818413\n",
      "Saved: 39: 17 June: 10:54:12 PM \n",
      "Saved: 40: 17 June: 10:54:14 PM \n",
      "Saved: 41: 17 June: 10:54:17 PM \n",
      "Saved bits and jobid:  43 : 17 June: 10:55:37 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:55:47.609959\n",
      "Saved: 42: 17 June: 10:57:51 PM \n",
      "Saved bits and jobid:  44 : 17 June: 10:58:12 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 22:58:22.481979\n",
      "Saved: 43: 17 June: 11:00:26 PM \n",
      "Saved bits and jobid:  45 : 17 June: 11:02:12 PM\n",
      "Saved bits and jobid:  46 : 17 June: 11:02:42 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:02:52.812631\n",
      "Saved: 44: 17 June: 11:04:55 PM \n",
      "Saved: 45: 17 June: 11:04:58 PM \n",
      "Saved bits and jobid:  47 : 17 June: 11:05:18 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:05:29.372637\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:07:33.515390\n",
      "Saved: 46: 17 June: 11:09:37 PM \n",
      "Saved: 47: 17 June: 11:09:39 PM \n",
      "Saved bits and jobid:  48 : 17 June: 11:09:56 PM\n",
      "Saved bits and jobid:  49 : 17 June: 11:15:10 PM\n",
      "Saved bits and jobid:  50 : 17 June: 11:15:36 PM\n",
      "Saved bits and jobid:  51 : 17 June: 11:16:04 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:16:15.479159\n",
      "Saved: 48: 17 June: 11:18:17 PM \n",
      "Saved: 49: 17 June: 11:18:19 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:18:23.576031\n",
      "Saved: 50: 17 June: 11:20:27 PM \n",
      "Saved bits and jobid:  52 : 17 June: 11:20:44 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:20:55.026433\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:23:00.096185\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:25:04.520920\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:27:09.082112\n",
      "Saved: 51: 17 June: 11:29:13 PM \n",
      "Saved bits and jobid:  53 : 17 June: 11:33:27 PM\n",
      "Saved bits and jobid:  54 : 17 June: 11:33:54 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:34:05.046226\n",
      "Saved: 52: 17 June: 11:36:07 PM \n",
      "Saved: 53: 17 June: 11:36:10 PM \n",
      "Saved bits and jobid:  55 : 17 June: 11:37:15 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:37:26.117027\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:39:30.730862\n",
      "Saved: 54: 17 June: 11:41:34 PM \n",
      "Saved: 55: 17 June: 11:41:37 PM \n",
      "Saved bits and jobid:  56 : 17 June: 11:41:58 PM\n",
      "Saved bits and jobid:  57 : 17 June: 11:46:51 PM\n",
      "Saved bits and jobid:  58 : 17 June: 11:47:19 PM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:47:29.507012\n",
      "Saved: 56: 17 June: 11:49:31 PM \n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:49:36.109025\n",
      "We have 3 jobs in queue, sleeping: 2020-06-17 23:51:39.709625\n",
      "Saved: 57: 17 June: 11:53:43 PM \n",
      "Saved: 58: 17 June: 11:53:48 PM \n",
      "Saved bits and jobid:  59 : 17 June: 11:54:12 PM\n",
      "Saved bits and jobid:  60 : 17 June: 11:54:40 PM\n",
      "Saved bits and jobid:  61 : 18 June: 12:01:41 AM\n",
      "Saved bits and jobid:  62 : 18 June: 12:02:10 AM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 jobs in queue, sleeping: 2020-06-18 00:02:22.030079\n",
      "Saved: 59: 18 June: 12:04:26 AM \n",
      "Saved: 60: 18 June: 12:04:29 AM \n",
      "Saved: 61: 18 June: 12:04:34 AM \n",
      "Saved bits and jobid:  63 : 18 June: 12:05:51 AM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-18 00:06:03.185327\n",
      "We have 3 jobs in queue, sleeping: 2020-06-18 00:08:07.673593\n",
      "Saved: 62: 18 June: 12:10:11 AM \n",
      "Saved bits and jobid:  64 : 18 June: 12:10:32 AM\n",
      "We have 3 jobs in queue, sleeping: 2020-06-18 00:10:43.617124\n",
      "Saved: 63: 18 June: 12:12:48 AM \n",
      "Saved: 64: 18 June: 12:12:52 AM \n",
      "Saved bits and jobid:  65 : 18 June: 12:15:56 AM\n",
      "Saved bits and jobid:  66 : 18 June: 12:16:26 AM\n"
     ]
    }
   ],
   "source": [
    "# You might want to do, say, 250*4 = 1000 sequence run.\n",
    "# The time stamps will show how long it took.\n",
    "# The text below is just an example of the output.\n",
    "(all_b,all_j,all_j_id) = sendAJobWhenOk(15,lengths,4,sets,backendToUse,0,250,saveFile=\"data/Melbourne15_17June2020_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printed at: 17 June: 07:28:34 AM \n",
      "ibmq_16_melbourne\n",
      "=================\n",
      "Configuration\n",
      "-------------\n",
      "    n_qubits: 15\n",
      "    operational: True\n",
      "    status_msg: active\n",
      "    pending_jobs: 48\n",
      "    backend_version: 2.1.0\n",
      "    basis_gates: ['id', 'u1', 'u2', 'u3', 'cx']\n",
      "    local: False\n",
      "    simulator: False\n",
      "    conditional: False\n",
      "    quantum_volume: None\n",
      "    online_date: 2018-11-06 05:00:00+00:00\n",
      "    url: None\n",
      "    meas_map: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]]\n",
      "    backend_name: ibmq_16_melbourne\n",
      "    description: 15 qubit device\n",
      "    n_registers: 1\n",
      "    credits_required: True\n",
      "    allow_object_storage: True\n",
      "    open_pulse: False\n",
      "    sample_name: albatross\n",
      "    coupling_map: [[0, 1], [0, 14], [1, 0], [1, 2], [1, 13], [2, 1], [2, 3], [2, 12], [3, 2], [3, 4], [3, 11], [4, 3], [4, 5], [4, 10], [5, 4], [5, 6], [5, 9], [6, 5], [6, 8], [7, 8], [8, 6], [8, 7], [8, 9], [9, 5], [9, 8], [9, 10], [10, 4], [10, 9], [10, 11], [11, 3], [11, 10], [11, 12], [12, 2], [12, 11], [12, 13], [13, 1], [13, 12], [13, 14], [14, 0], [14, 13]]\n",
      "    memory: True\n",
      "    max_shots: 8192\n",
      "    max_experiments: 75\n",
      "    allow_q_object: True\n",
      "\n",
      "Qubits [Name / Freq / T1 / T2 / U1 err / U2 err / U3 err / Readout err]\n",
      "-----------------------------------------------------------------------\n",
      "    Q0 / 5.10016 GHz / 77.75977 µs / 26.10151 µs / 0 / 0.00068 / 0.00136 / 0.028\n",
      "    Q1 / 5.23846 GHz / 54.41469 µs / 78.42977 µs / 0 / 0.00154 / 0.00307 / 0.0775\n",
      "    Q2 / 5.03282 GHz / 32.35966 µs / 57.51078 µs / 0 / 0.0014 / 0.0028 / 0.026\n",
      "    Q3 / 4.89609 GHz / 73.12281 µs / 65.76894 µs / 0 / 0.0004 / 0.0008 / 0.042\n",
      "    Q4 / 5.02624 GHz / 56.57732 µs / 6.11544 µs / 0 / 0.0064 / 0.01276 / 0.0885\n",
      "    Q5 / 5.06678 GHz / 17.86191 µs / 33.39834 µs / 0 / 0.0025 / 0.005 / 0.0655\n",
      "    Q6 / 4.92405 GHz / 86.27347 µs / 117.33066 µs / 0 / 0.00084 / 0.00168 / 0.1225\n",
      "    Q7 / 4.9745 GHz / 42.29254 µs / 78.90615 µs / 0 / 0.00177 / 0.00353 / 0.0515\n",
      "    Q8 / 4.74067 GHz / 42.79059 µs / 66.43545 µs / 0 / 0.00081 / 0.00162 / 0.288\n",
      "    Q9 / 4.96328 GHz / 39.47501 µs / 68.04136 µs / 0 / 0.00157 / 0.00314 / 0.037\n",
      "    Q10 / 4.94521 GHz / 63.66671 µs / 62.79309 µs / 0 / 0.00119 / 0.00237 / 0.0255\n",
      "    Q11 / 5.00483 GHz / 63.99346 µs / 70.85867 µs / 0 / 0.00165 / 0.0033 / 0.028\n",
      "    Q12 / 4.76 GHz / 81.9022 µs / 51.94175 µs / 0 / 0.00064 / 0.00128 / 0.0335\n",
      "    Q13 / 4.96832 GHz / 27.89627 µs / 45.10811 µs / 0 / 0.00166 / 0.00332 / 0.087\n",
      "    Q14 / 5.00148 GHz / 44.44256 µs / 33.07061 µs / 0 / 0.00123 / 0.00247 / 0.055\n",
      "\n",
      "Multi-Qubit Gates [Name / Type / Gate Error]\n",
      "--------------------------------------------\n",
      "    cx0_1 / cx / 0.02908\n",
      "    cx0_14 / cx / 0.02685\n",
      "    cx1_0 / cx / 0.02908\n",
      "    cx1_2 / cx / 0.01464\n",
      "    cx1_13 / cx / 0.05671\n",
      "    cx2_1 / cx / 0.01464\n",
      "    cx2_3 / cx / 0.02818\n",
      "    cx2_12 / cx / 0.03984\n",
      "    cx3_2 / cx / 0.02818\n",
      "    cx3_4 / cx / 0.04984\n",
      "    cx3_11 / cx / 0.03462\n",
      "    cx4_3 / cx / 0.04984\n",
      "    cx4_5 / cx / 0.09061\n",
      "    cx4_10 / cx / 0.06984\n",
      "    cx5_4 / cx / 0.09061\n",
      "    cx5_6 / cx / 0.04796\n",
      "    cx5_9 / cx / 0.03672\n",
      "    cx6_5 / cx / 0.04796\n",
      "    cx6_8 / cx / 0.04741\n",
      "    cx7_8 / cx / 0.03579\n",
      "    cx8_6 / cx / 0.04741\n",
      "    cx8_7 / cx / 0.03579\n",
      "    cx8_9 / cx / 0.03795\n",
      "    cx9_5 / cx / 0.03672\n",
      "    cx9_8 / cx / 0.03795\n",
      "    cx9_10 / cx / 0.02926\n",
      "    cx10_4 / cx / 0.06984\n",
      "    cx10_9 / cx / 0.02926\n",
      "    cx10_11 / cx / 0.02476\n",
      "    cx11_3 / cx / 0.03462\n",
      "    cx11_10 / cx / 0.02476\n",
      "    cx11_12 / cx / 0.02542\n",
      "    cx12_2 / cx / 0.03984\n",
      "    cx12_11 / cx / 0.02542\n",
      "    cx12_13 / cx / 0.02774\n",
      "    cx13_1 / cx / 0.05671\n",
      "    cx13_12 / cx / 0.02774\n",
      "    cx13_14 / cx / 0.04844\n",
      "    cx14_0 / cx / 0.02685\n",
      "    cx14_13 / cx / 0.04844\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(\"Printed at: \" + now.strftime(\"%d %B: %r \"))\n",
    "qiskit.tools.backend_monitor(backendToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it.\n",
    "\n",
    "After this has run, I believe everything is saved in the pickles. The other workbook shows how to extract the data from the pickles that we need for the Efficient Learning algorithm. You could of course use the list of jobs you have here etc, up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

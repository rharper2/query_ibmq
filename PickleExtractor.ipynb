{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qiskit\n",
    "\n",
    "Takes pickles saved from qubit experiments.\n",
    "\n",
    "This is geared towards extracting the data from 1 and 2 qubit circuits on all the qubits simultaneously as per [Efficient Learning of Quantum Noise](https://arxiv.org/abs/1907.13022).\n",
    "\n",
    "\n",
    "As per ususal some of the specifics are going to go out of date pretty quickly but the qasm code etc should be easily adapted. \n",
    "\n",
    "My experience, however, is that sometimes the pickles can't be unpickled with higher versions. It might be possbile to use GitHub to load an older version. However at the end of this workbook all the relevant data is stored as csv so that is all I need.\n",
    "\n",
    "This should run on an appropriate version python3 system where you have installed qiskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T00:04:16.313210Z",
     "start_time": "2018-09-29T00:04:14.460647Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit import __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import IBMQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random is needed for the initial X gates. In order to minimise SPAM (i.e. B=0.5/0.25 in old parlance)\n",
    "# a random mix of which state we return it to is best. Could seed this if you want.\n",
    "import random\n",
    "# The rest are for saving and logging.\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just makes it easier to pickle the bit pattern used to randomise the 'expected' measurements\n",
    "class savedPair:\n",
    "    def __init__(self, bits,result):\n",
    "        self.bits = bits\n",
    "        self.result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringifyBits(bits):\n",
    "    \"\"\" Takes an array of bits (i.e. 1 or 0) and turns them into a string \"\"\"\n",
    "    return [''.join([str(x) for x in i]) for i in bits]\n",
    "\n",
    "def mashable(s1,s2):\n",
    "    \"\"\" XOR on strings \"\"\"\n",
    "    s3 = ''\n",
    "    for i in range(len(s1)):\n",
    "        if s1[i] == s2[i]:\n",
    "            s3 = s3 + '0'\n",
    "        else:\n",
    "            s3 = s3 + '1'\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 extract the  pickles.\n",
    "\n",
    "This is a bit fiddly - extraction depends on the exact forms saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data was saved as follows:\n",
    "\n",
    "import glob \n",
    "dirlist = glob.glob('./data/Melbourne15_17June2020_[0-9]*.pickle')\n",
    "len(dirlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up one just so we can get the various metrics\n",
    "with open(dirlist[0],'rb') as f:\n",
    "    srSp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_pickle = len(srSp.bits)\n",
    "sequences_per_batch = len(srSp.bits[0])\n",
    "number_of_qubits = len(srSp.bits[0][0])\n",
    "bit_format = \"{0:0\"+str(number_of_qubits)+\"b}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because we only have 15 qubits, we can just create the global probability distribution\n",
    "\n",
    "- If we have more than, say, 28 then save different counts and we can reconstruct from marginalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create our empty dicts of each possible key\n",
    "lengthDicts = [{} for i in range(sequences_per_batch)]\n",
    "for ld in lengthDicts:\n",
    "    for i in range(0,2**number_of_qubits):\n",
    "        ld[bit_format.format(i)]=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resultDict = {}\n",
    "scale = 16 ## equals to hexadecimal\n",
    "count = 0\n",
    "for i in dirlist:\n",
    "    with open(i,'rb') as f:\n",
    "       # print(\"File: \",i)\n",
    "        srSps = pickle.load(f)\n",
    "        # We saved this as batches so deal with each batch seperately\n",
    "        for i in range(len(srSps.bits)):\n",
    "            bits = stringifyBits(srSps.bits[i])\n",
    "            counts = srSps.result.get_counts()\n",
    "            for idx in range(0,sequences_per_batch):\n",
    "                # note we need to reverse the mask - IBM stores Q0 as right hand bit.\n",
    "                mask = bits[idx][::-1]\n",
    "                resultDict = counts[i*sequences_per_batch+idx]\n",
    "                if not sum(resultDict.values()) == 1024:\n",
    "                        print(\"Error with counts of \",i)\n",
    "                for actKeys in resultDict.keys():\n",
    "                        countForKey = resultDict[actKeys]\n",
    "                        lengthDicts[idx][mashable(actKeys,mask)] += countForKey      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedKeys = [bit_format.format(i) for i in range(0,2**15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000000 -> 252018\n",
      "000000000000001 -> 9048\n",
      "000000000000010 -> 23015\n",
      "000000000000011 -> 924\n",
      "000000000000100 -> 9581\n",
      "000000000000101 -> 372\n",
      "000000000000110 -> 899\n",
      "000000000000111 -> 36\n",
      "000000000001000 -> 13133\n",
      "000000000001001 -> 426\n",
      "================\n",
      "000000000000000 -> 210502\n",
      "000000000000001 -> 7485\n",
      "000000000000010 -> 21194\n",
      "000000000000011 -> 812\n",
      "000000000000100 -> 9647\n",
      "000000000000101 -> 308\n",
      "000000000000110 -> 1044\n",
      "000000000000111 -> 40\n",
      "000000000001000 -> 11856\n",
      "000000000001001 -> 411\n",
      "================\n",
      "000000000000000 -> 171183\n",
      "000000000000001 -> 6491\n",
      "000000000000010 -> 18728\n",
      "000000000000011 -> 802\n",
      "000000000000100 -> 9228\n",
      "000000000000101 -> 337\n",
      "000000000000110 -> 1069\n",
      "000000000000111 -> 48\n",
      "000000000001000 -> 9693\n",
      "000000000001001 -> 396\n",
      "================\n",
      "000000000000000 -> 141534\n",
      "000000000000001 -> 5959\n",
      "000000000000010 -> 16801\n",
      "000000000000011 -> 811\n",
      "000000000000100 -> 8189\n",
      "000000000000101 -> 370\n",
      "000000000000110 -> 1136\n",
      "000000000000111 -> 51\n",
      "000000000001000 -> 8136\n",
      "000000000001001 -> 352\n",
      "================\n",
      "000000000000000 -> 115142\n",
      "000000000000001 -> 5261\n",
      "000000000000010 -> 14940\n",
      "000000000000011 -> 818\n",
      "000000000000100 -> 8267\n",
      "000000000000101 -> 362\n",
      "000000000000110 -> 1132\n",
      "000000000000111 -> 53\n",
      "000000000001000 -> 7057\n",
      "000000000001001 -> 335\n",
      "================\n",
      "000000000000000 -> 79313\n",
      "000000000000001 -> 4050\n",
      "000000000000010 -> 11554\n",
      "000000000000011 -> 763\n",
      "000000000000100 -> 6811\n",
      "000000000000101 -> 377\n",
      "000000000000110 -> 1074\n",
      "000000000000111 -> 78\n",
      "000000000001000 -> 5348\n",
      "000000000001001 -> 279\n",
      "================\n",
      "000000000000000 -> 48993\n",
      "000000000000001 -> 3016\n",
      "000000000000010 -> 8580\n",
      "000000000000011 -> 666\n",
      "000000000000100 -> 5682\n",
      "000000000000101 -> 371\n",
      "000000000000110 -> 1033\n",
      "000000000000111 -> 77\n",
      "000000000001000 -> 3593\n",
      "000000000001001 -> 223\n",
      "================\n",
      "000000000000000 -> 32433\n",
      "000000000000001 -> 2446\n",
      "000000000000010 -> 6625\n",
      "000000000000011 -> 593\n",
      "000000000000100 -> 4545\n",
      "000000000000101 -> 345\n",
      "000000000000110 -> 1027\n",
      "000000000000111 -> 80\n",
      "000000000001000 -> 2616\n",
      "000000000001001 -> 214\n",
      "================\n",
      "000000000000000 -> 20912\n",
      "000000000000001 -> 1677\n",
      "000000000000010 -> 5000\n",
      "000000000000011 -> 435\n",
      "000000000000100 -> 3495\n",
      "000000000000101 -> 293\n",
      "000000000000110 -> 917\n",
      "000000000000111 -> 99\n",
      "000000000001000 -> 1766\n",
      "000000000001001 -> 154\n",
      "================\n",
      "000000000000000 -> 15085\n",
      "000000000000001 -> 1423\n",
      "000000000000010 -> 3987\n",
      "000000000000011 -> 490\n",
      "000000000000100 -> 3020\n",
      "000000000000101 -> 262\n",
      "000000000000110 -> 848\n",
      "000000000000111 -> 96\n",
      "000000000001000 -> 1451\n",
      "000000000001001 -> 141\n",
      "================\n",
      "000000000000000 -> 9739\n",
      "000000000000001 -> 1049\n",
      "000000000000010 -> 3038\n",
      "000000000000011 -> 363\n",
      "000000000000100 -> 2348\n",
      "000000000000101 -> 262\n",
      "000000000000110 -> 753\n",
      "000000000000111 -> 84\n",
      "000000000001000 -> 949\n",
      "000000000001001 -> 128\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "# Sanity check we are expecting 000.0000 to be the error free result\n",
    "# When we have a low sequence of gates, the count of 000.000 should dominate.\n",
    "\n",
    "for idx in range(sequences_per_batch):\n",
    "    for i in orderedKeys[0:10]:\n",
    "        print(i,\"->\",lengthDicts[idx][i])\n",
    "    print(\"================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is now in the format we need for the paper. We can write it out as a string of bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedKeys = [bit_format.format(i) for i in range(0,2**number_of_qubits)]\n",
    "## And save\n",
    "import csv \n",
    "with open('./data/results15_17June2020.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile,delimiter=',')\n",
    "    for i in lengthDicts:\n",
    "        row = [i[k] for k in orderedKeys]\n",
    "        writer.writerow(row)\n",
    "    \n",
    "### and analyse (for me, just now, in Julia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If the number of qubits is too large to expressly write down the $2^n$ numbers, then we should write each shot separately, once we 'de-hash' the x-gates. Then if, say, we had a 1 million measurements we would have a file of 1 million outcomes seen.\n",
    "\n",
    "The algorithm then proceeds by marginalising the outcomes as we read them (one of the workbooks in (https://github.com/rharper2/Juqst.jl) has more detail of this depending on the factorization of our GRF ansatz. This will have to be done multiple times depending on the details of the GRF factorization used, but will be scalable. A simplified example is also given in one of the workbooks. More details will (hopefully) be forthcoming."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
